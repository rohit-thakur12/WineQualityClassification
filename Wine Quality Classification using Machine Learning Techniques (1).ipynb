{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_white = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',\n",
    "                sep=';')\n",
    "\n",
    "df_red = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',\n",
    "                sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to model immbalanced dataset\n",
    "# which model performs best\n",
    "# how to do benchmarking\n",
    "# conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_red,df_white])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           1599 non-null float64\n",
      "volatile acidity        1599 non-null float64\n",
      "citric acid             1599 non-null float64\n",
      "residual sugar          1599 non-null float64\n",
      "chlorides               1599 non-null float64\n",
      "free sulfur dioxide     1599 non-null float64\n",
      "total sulfur dioxide    1599 non-null float64\n",
      "density                 1599 non-null float64\n",
      "pH                      1599 non-null float64\n",
      "sulphates               1599 non-null float64\n",
      "alcohol                 1599 non-null float64\n",
      "quality                 1599 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "df_red.info() \n",
    "\n",
    "# type of data: float64(11), int64(1)\n",
    "# number of rows: 1599 entries\n",
    "# col count: total 12 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           4898 non-null float64\n",
      "volatile acidity        4898 non-null float64\n",
      "citric acid             4898 non-null float64\n",
      "residual sugar          4898 non-null float64\n",
      "chlorides               4898 non-null float64\n",
      "free sulfur dioxide     4898 non-null float64\n",
      "total sulfur dioxide    4898 non-null float64\n",
      "density                 4898 non-null float64\n",
      "pH                      4898 non-null float64\n",
      "sulphates               4898 non-null float64\n",
      "alcohol                 4898 non-null float64\n",
      "quality                 4898 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "df_white.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6497 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           6497 non-null float64\n",
      "volatile acidity        6497 non-null float64\n",
      "citric acid             6497 non-null float64\n",
      "residual sugar          6497 non-null float64\n",
      "chlorides               6497 non-null float64\n",
      "free sulfur dioxide     6497 non-null float64\n",
      "total sulfur dioxide    6497 non-null float64\n",
      "density                 6497 non-null float64\n",
      "pH                      6497 non-null float64\n",
      "sulphates               6497 non-null float64\n",
      "alcohol                 6497 non-null float64\n",
      "quality                 6497 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 659.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.quality) #checking unique values from the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the data type\n",
    "df['quality'] = pd.Categorical(df['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6497 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           6497 non-null float64\n",
      "volatile acidity        6497 non-null float64\n",
      "citric acid             6497 non-null float64\n",
      "residual sugar          6497 non-null float64\n",
      "chlorides               6497 non-null float64\n",
      "free sulfur dioxide     6497 non-null float64\n",
      "total sulfur dioxide    6497 non-null float64\n",
      "density                 6497 non-null float64\n",
      "pH                      6497 non-null float64\n",
      "sulphates               6497 non-null float64\n",
      "alcohol                 6497 non-null float64\n",
      "quality                 6497 non-null category\n",
      "dtypes: category(1), float64(11)\n",
      "memory usage: 615.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol quality  \n",
       "0      9.4       5  \n",
       "1      9.8       5  \n",
       "2      9.8       5  \n",
       "3      9.8       6  \n",
       "4      9.4       5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ecde365748>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.distplot(df['fixed acidity']) # this feature is marginally right skewed, some outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ecde1a5cc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.distplot(df['volatile acidity']) # this feature is right skewed, some outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ecde0a9668>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(df['fixed acidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the number of outliers less than 10%, it is ok\n",
    "# if it is more than 10%, you need to cap the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6497.000000\n",
       "mean        7.215307\n",
       "std         1.296434\n",
       "min         3.800000\n",
       "25%         6.400000\n",
       "50%         7.000000\n",
       "75%         7.700000\n",
       "max        15.900000\n",
       "Name: fixed acidity, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fixed acidity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2999999999999998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IQR = df['fixed acidity'].describe()[6] - df['fixed acidity'].describe()[4]\n",
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCL = df['residual sugar'].describe()[6] + (1.5 * IQR)\n",
    "LCL = df['residual sugar'].describe()[4] - (1.5 * IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1499999999999997"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.049999999999999"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.38 % outlier on the right hand side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.53162998306911"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[(df['residual sugar'] > UCL)].shape[0]/df.shape[0])*100 # 5.38 % outlier on the right hand side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[(df['residual sugar'] < LCL)].shape[0]/df.shape[0])*100 # 0.10 % outlier on the left hand side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.53162998306911"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df[(df['residual sugar'] > UCL)].shape[0]/df.shape[0])*100) + (df[(df['residual sugar'] < LCL)].shape[0]/df.shape[0])*100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness of the target or dependent column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    0.436509\n",
       "5    0.329075\n",
       "7    0.166077\n",
       "4    0.033246\n",
       "8    0.029706\n",
       "3    0.004618\n",
       "9    0.000770\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quality.value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2836\n",
       "5    2138\n",
       "7    1079\n",
       "4     216\n",
       "8     193\n",
       "3      30\n",
       "9       5\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class balance by combining to the nearest class\n",
    "df['quality'] = df.quality.replace({3: 4, 9: 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2836\n",
       "5    2138\n",
       "7    1079\n",
       "4     246\n",
       "8     198\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'residual sugar' should be considered to be removed from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5197, 11), (1300, 11), (5197,), (1300,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['fixed acidity', 'volatile acidity', 'citric acid', \n",
    "        'residual sugar',\n",
    "       'chlorides', \n",
    "        'free sulfur dioxide', \n",
    "        'total sulfur dioxide', 'density',\n",
    "       'pH', 'sulphates', 'alcohol']].as_matrix()\n",
    "Y = df['quality'].as_matrix()\n",
    "\n",
    "Xtrain, Xtest,Ytrain, Ytest = train_test_split(X,Y,test_size=0.20,\n",
    "                                               random_state=1234)\n",
    "\n",
    "Xtrain.shape, Xtest.shape, Ytrain.shape,Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.539157206080431"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(Xtrain,Ytrain)\n",
    "#0.539157206080431(with 11 features)\n",
    "#0.5326149701751011(with 9 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5376923076923077"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(Xtest,Ytest)\n",
    "#0.5376923076923077(with 11 features)\n",
    "#0.53 (with 9 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existence of Outliers has no role in predicting the classes of wine quality, no impact on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02888043,  3.10953556, -0.58305411, -3.99748253, -1.8874633 ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.89094191e-02,  2.61170832e+00, -3.07054607e-01,\n",
       "        -4.94147896e-02, -2.38685142e-02, -5.98680065e-02,\n",
       "         1.10495961e-02,  1.51580382e-02, -2.51437520e-04,\n",
       "        -1.85777549e+00, -2.28835519e-01],\n",
       "       [ 2.20475230e-02,  3.08409266e+00,  6.51345040e-01,\n",
       "        -4.34536025e-02,  8.06738614e-01, -1.12586056e-02,\n",
       "         6.00552950e-03,  3.10960178e+00,  3.15765996e-01,\n",
       "        -1.17345382e+00, -8.66923137e-01],\n",
       "       [ 3.13034096e-02, -2.35671594e+00, -6.46893636e-01,\n",
       "         1.89924159e-02,  6.07422872e-01,  6.56770628e-03,\n",
       "        -3.45068071e-03, -5.21156074e-01,  7.22696321e-02,\n",
       "         3.71554777e-01,  1.19332575e-01],\n",
       "       [ 1.05825140e-02, -3.30971731e+00, -3.52445207e-01,\n",
       "         3.62553552e-02, -1.93782386e+00,  1.06903981e-02,\n",
       "        -5.62738605e-03, -4.00264386e+00, -2.85222274e-01,\n",
       "         1.56627242e+00,  7.15263807e-01],\n",
       "       [-3.78657146e-01, -9.95274844e-01,  6.65596813e-01,\n",
       "         5.87086454e-02, -3.63331859e-01,  2.66073527e-02,\n",
       "        -8.05203852e-03, -1.88451744e+00, -1.76196015e+00,\n",
       "         2.65148143e-01,  7.56259969e-01]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_quality = model1.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  32,  14,   0,   0],\n",
       "       [  0, 267, 151,   1,   0],\n",
       "       [  0, 140, 414,  10,   0],\n",
       "       [  0,  15, 198,  18,   0],\n",
       "       [  0,   2,  37,   1,   0]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Ytest,pred_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00        46\n",
      "           5       0.59      0.64      0.61       419\n",
      "           6       0.51      0.73      0.60       564\n",
      "           7       0.60      0.08      0.14       231\n",
      "           8       0.00      0.00      0.00        40\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      1300\n",
      "   macro avg       0.34      0.29      0.27      1300\n",
      "weighted avg       0.52      0.54      0.48      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Ytest,pred_quality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search to identify the best parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(multi_class='auto',max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to perform grid search with cross validation and returns the accuracy values\n",
    "def GridSearch_BestParam(X, y, clf, param_grid,cv=10):\n",
    "    grid_search = GridSearchCV(clf,\n",
    "                              param_grid=param_grid,\n",
    "                              cv=cv)\n",
    "    start= time()\n",
    "    grid_search.fit(X,y)\n",
    "    top_params=grid_search.cv_results_\n",
    "    return top_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='auto',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"C\": [0.001,0.05,0.1],\n",
    "             #\"penalty\": ['l2','l1'],\n",
    "              'solver':['newton-cg', 'lbfgs', 'liblinear']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 0.62108941,  3.06842277,  0.05568936,  1.3952379 , 12.09944973,\n",
      "        0.10077124,  1.52381501, 12.18592527,  0.10462375]), 'std_fit_time': array([0.05271306, 0.55015604, 0.00248236, 0.14058134, 1.69114784,\n",
      "       0.00815809, 0.10158797, 1.58957488, 0.00447827]), 'mean_score_time': array([0.00040038, 0.00030019, 0.00045052, 0.0004003 , 0.00045042,\n",
      "       0.0004508 , 0.00030012, 0.00035026, 0.00045073]), 'std_score_time': array([0.00030027, 0.00024511, 0.00026974, 0.00020015, 0.00015014,\n",
      "       0.00015027, 0.00024505, 0.00032049, 0.00015024]), 'param_C': masked_array(data=[0.001, 0.001, 0.001, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_solver': masked_array(data=['newton-cg', 'lbfgs', 'liblinear', 'newton-cg',\n",
      "                   'lbfgs', 'liblinear', 'newton-cg', 'lbfgs',\n",
      "                   'liblinear'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.001, 'solver': 'newton-cg'}, {'C': 0.001, 'solver': 'lbfgs'}, {'C': 0.001, 'solver': 'liblinear'}, {'C': 0.05, 'solver': 'newton-cg'}, {'C': 0.05, 'solver': 'lbfgs'}, {'C': 0.05, 'solver': 'liblinear'}, {'C': 0.1, 'solver': 'newton-cg'}, {'C': 0.1, 'solver': 'lbfgs'}, {'C': 0.1, 'solver': 'liblinear'}], 'split0_test_score': array([0.49712092, 0.49712092, 0.45489443, 0.52591171, 0.53358925,\n",
      "       0.50863724, 0.52399232, 0.52399232, 0.51247601]), 'split1_test_score': array([0.52207294, 0.52015355, 0.44913628, 0.55278311, 0.5547025 ,\n",
      "       0.54510557, 0.55854127, 0.55278311, 0.55662188]), 'split2_test_score': array([0.49038462, 0.49038462, 0.45576923, 0.54038462, 0.54230769,\n",
      "       0.52115385, 0.53461538, 0.53846154, 0.52115385]), 'split3_test_score': array([0.49615385, 0.49615385, 0.45769231, 0.53461538, 0.53269231,\n",
      "       0.52307692, 0.55384615, 0.55769231, 0.53076923]), 'split4_test_score': array([0.51153846, 0.51538462, 0.46923077, 0.53461538, 0.53461538,\n",
      "       0.51730769, 0.54230769, 0.54038462, 0.52884615]), 'split5_test_score': array([0.51153846, 0.51153846, 0.44807692, 0.53461538, 0.53461538,\n",
      "       0.51538462, 0.53461538, 0.53269231, 0.51538462]), 'split6_test_score': array([0.49423077, 0.49423077, 0.47692308, 0.51730769, 0.51730769,\n",
      "       0.51346154, 0.53461538, 0.53269231, 0.50576923]), 'split7_test_score': array([0.50384615, 0.50384615, 0.46153846, 0.56346154, 0.56730769,\n",
      "       0.54615385, 0.56538462, 0.56923077, 0.55769231]), 'split8_test_score': array([0.5019305 , 0.5019305 , 0.43436293, 0.53281853, 0.53088803,\n",
      "       0.51544402, 0.53667954, 0.53474903, 0.52509653]), 'split9_test_score': array([0.52417795, 0.52611219, 0.47001934, 0.54352031, 0.53965184,\n",
      "       0.54545455, 0.54352031, 0.54738878, 0.5377176 ]), 'mean_test_score': array([0.50529151, 0.50567635, 0.45776409, 0.53800269, 0.53877237,\n",
      "       0.52511064, 0.54281316, 0.54300558, 0.52915143]), 'std_test_score': array([0.0110627 , 0.01140505, 0.01178597, 0.01240892, 0.01305325,\n",
      "       0.01389942, 0.01214349, 0.01299874, 0.01656232]), 'rank_test_score': array([8, 7, 9, 4, 3, 6, 2, 1, 5]), 'split0_train_score': array([0.51154833, 0.51154833, 0.46043627, 0.5427716 , 0.54426861,\n",
      "       0.52865697, 0.55047049, 0.54875962, 0.53229256]), 'split1_train_score': array([0.50384944, 0.50384944, 0.45786997, 0.53934987, 0.53635586,\n",
      "       0.52437981, 0.54255774, 0.54384089, 0.52502139]), 'split2_train_score': array([0.50887321, 0.5086594 , 0.45905495, 0.5400898 , 0.53987599,\n",
      "       0.52939919, 0.54372461, 0.54415223, 0.52982681]), 'split3_train_score': array([0.50673509, 0.50673509, 0.45948257, 0.54158649, 0.5418003 ,\n",
      "       0.52405388, 0.54650417, 0.54457986, 0.53110969]), 'split4_train_score': array([0.5086594 , 0.50630746, 0.46482788, 0.54415223, 0.54372461,\n",
      "       0.52897156, 0.54607654, 0.54586273, 0.53282018]), 'split5_train_score': array([0.5086594 , 0.50951465, 0.45777208, 0.54158649, 0.54073124,\n",
      "       0.52982681, 0.54500748, 0.54607654, 0.53110969]), 'split6_train_score': array([0.5103699 , 0.50972846, 0.46012401, 0.54308317, 0.54244174,\n",
      "       0.52747488, 0.54671798, 0.54650417, 0.53153731]), 'split7_train_score': array([0.50203122, 0.50245884, 0.45798589, 0.53923455, 0.53709643,\n",
      "       0.52298482, 0.54244174, 0.5418003 , 0.52854394]), 'split8_train_score': array([0.50673221, 0.50651849, 0.46420175, 0.54285104, 0.54327848,\n",
      "       0.52575337, 0.54562941, 0.54434708, 0.53152383]), 'split9_train_score': array([0.50512821, 0.5042735 , 0.45918803, 0.54230769, 0.54252137,\n",
      "       0.52820513, 0.54700855, 0.5482906 , 0.53098291]), 'mean_train_score': array([0.50725864, 0.50695937, 0.46009434, 0.54170129, 0.54120946,\n",
      "       0.52697064, 0.54561387, 0.5454214 , 0.53047683]), 'std_train_score': array([0.00280448, 0.00275601, 0.00237607, 0.0015808 , 0.00256973,\n",
      "       0.00235095, 0.00226047, 0.00200639, 0.00214445])}\n"
     ]
    }
   ],
   "source": [
    "top_para = GridSearch_BestParam(Xtrain,Ytrain, model2, param_grid, cv=10)\n",
    "print (top_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_score = sorted(top_para,key=itemgetter(1), reverse=True)\n",
    "len(top_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    8\n",
       "1    7\n",
       "2    9\n",
       "3    4\n",
       "4    3\n",
       "5    6\n",
       "6    2\n",
       "7    1\n",
       "8    5\n",
       "Name: rank_test_score, dtype: int32"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(top_para)['rank_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.523815</td>\n",
       "      <td>0.101588</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'C': 0.1, 'solver': 'newton-cg'}</td>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.558541</td>\n",
       "      <td>0.534615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543725</td>\n",
       "      <td>0.546504</td>\n",
       "      <td>0.546077</td>\n",
       "      <td>0.545007</td>\n",
       "      <td>0.546718</td>\n",
       "      <td>0.542442</td>\n",
       "      <td>0.545629</td>\n",
       "      <td>0.547009</td>\n",
       "      <td>0.545614</td>\n",
       "      <td>0.002260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.185925</td>\n",
       "      <td>1.589575</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'C': 0.1, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.552783</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544152</td>\n",
       "      <td>0.544580</td>\n",
       "      <td>0.545863</td>\n",
       "      <td>0.546077</td>\n",
       "      <td>0.546504</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.544347</td>\n",
       "      <td>0.548291</td>\n",
       "      <td>0.545421</td>\n",
       "      <td>0.002006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.104624</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 0.1, 'solver': 'liblinear'}</td>\n",
       "      <td>0.512476</td>\n",
       "      <td>0.556622</td>\n",
       "      <td>0.521154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529827</td>\n",
       "      <td>0.531110</td>\n",
       "      <td>0.532820</td>\n",
       "      <td>0.531110</td>\n",
       "      <td>0.531537</td>\n",
       "      <td>0.528544</td>\n",
       "      <td>0.531524</td>\n",
       "      <td>0.530983</td>\n",
       "      <td>0.530477</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "6       1.523815      0.101588         0.000300        0.000245     0.1   \n",
       "7      12.185925      1.589575         0.000350        0.000320     0.1   \n",
       "8       0.104624      0.004478         0.000451        0.000150     0.1   \n",
       "\n",
       "  param_solver                             params  split0_test_score  \\\n",
       "6    newton-cg  {'C': 0.1, 'solver': 'newton-cg'}           0.523992   \n",
       "7        lbfgs      {'C': 0.1, 'solver': 'lbfgs'}           0.523992   \n",
       "8    liblinear  {'C': 0.1, 'solver': 'liblinear'}           0.512476   \n",
       "\n",
       "   split1_test_score  split2_test_score  ...  split2_train_score  \\\n",
       "6           0.558541           0.534615  ...            0.543725   \n",
       "7           0.552783           0.538462  ...            0.544152   \n",
       "8           0.556622           0.521154  ...            0.529827   \n",
       "\n",
       "   split3_train_score  split4_train_score  split5_train_score  \\\n",
       "6            0.546504            0.546077            0.545007   \n",
       "7            0.544580            0.545863            0.546077   \n",
       "8            0.531110            0.532820            0.531110   \n",
       "\n",
       "   split6_train_score  split7_train_score  split8_train_score  \\\n",
       "6            0.546718            0.542442            0.545629   \n",
       "7            0.546504            0.541800            0.544347   \n",
       "8            0.531537            0.528544            0.531524   \n",
       "\n",
       "   split9_train_score  mean_train_score  std_train_score  \n",
       "6            0.547009          0.545614         0.002260  \n",
       "7            0.548291          0.545421         0.002006  \n",
       "8            0.530983          0.530477         0.002144  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(top_para).tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = LogisticRegression(C=0.1, \n",
    "                                solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5324225514720031"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.score(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5269230769230769"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitations: why we are not getting a good accuracy?\n",
    "# class imbalance\n",
    "# outliers\n",
    "# interaction between features\n",
    "# non-linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    0.436509\n",
       "5    0.329075\n",
       "7    0.166077\n",
       "4    0.037864\n",
       "8    0.030476\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quality.value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6497 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           6497 non-null float64\n",
      "volatile acidity        6497 non-null float64\n",
      "citric acid             6497 non-null float64\n",
      "residual sugar          6497 non-null float64\n",
      "chlorides               6497 non-null float64\n",
      "free sulfur dioxide     6497 non-null float64\n",
      "total sulfur dioxide    6497 non-null float64\n",
      "density                 6497 non-null float64\n",
      "pH                      6497 non-null float64\n",
      "sulphates               6497 non-null float64\n",
      "alcohol                 6497 non-null float64\n",
      "quality                 6497 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 659.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear model: Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeClassifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt.score(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5938461538461538"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"criterion\": [\"gini\",\"entropy\"],\n",
    "             \"min_samples_split\": [10,20,30],\n",
    "             \"max_depth\": [5,7],\n",
    "             \"min_samples_leaf\":[10,15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvDT = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_para = GridSearch_BestParam(Xtrain,Ytrain, cvDT, param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "dd = pd.DataFrame(top_para)[['rank_test_score','params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 7,\n",
       " 'min_samples_leaf': 15,\n",
       " 'min_samples_split': 10}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[dd.rank_test_score==1].values.flatten()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 7,\n",
       " 'min_samples_leaf': 15,\n",
       " 'min_samples_split': 10}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramCV = dd[dd.rank_test_score==1].values.flatten()[1]\n",
    "paramCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=15, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dtmodel = DecisionTreeClassifier(max_depth=paramCV['max_depth'], \n",
    "                                           min_samples_split=paramCV['min_samples_split'],\n",
    "                                    min_samples_leaf=paramCV['min_samples_leaf'],\n",
    "                                   criterion=paramCV['criterion'])\n",
    "best_dtmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=15, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dtmodel.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6170867808350972"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dtmodel.score(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5592307692307692"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dtmodel.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05632659, 0.16809601, 0.01443399, 0.04830826, 0.07754538,\n",
       "       0.07510672, 0.03293075, 0.01247498, 0.0441276 , 0.07107017,\n",
       "       0.39957954])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dtmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp['variable'] = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
    "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "       'pH', 'sulphates', 'alcohol']\n",
    "temp['VarImp'] = np.round(best_dtmodel.feature_importances_,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VarImp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volatile acidity</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chlorides</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>free sulfur dioxide</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sulphates</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fixed acidity</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>residual sugar</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pH</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total sulfur dioxide</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>citric acid</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>density</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                variable  VarImp\n",
       "10               alcohol    0.40\n",
       "1       volatile acidity    0.17\n",
       "4              chlorides    0.08\n",
       "5    free sulfur dioxide    0.08\n",
       "9              sulphates    0.07\n",
       "0          fixed acidity    0.06\n",
       "3         residual sugar    0.05\n",
       "8                     pH    0.04\n",
       "6   total sulfur dioxide    0.03\n",
       "2            citric acid    0.01\n",
       "7                density    0.01"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.sort_index(by='VarImp',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-fe84689fbf8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, pydotplus\n",
    "from sklearn import tree, metrics, model_selection, preprocessing\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(best_dtmodel, \n",
    "                                out_file=None, \n",
    "                                filled=True, \n",
    "                                rounded=True,\n",
    "                                )\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "display(Image(graph.create_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.write_png('output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Example of Bias and variance through IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/639388c2cbc2120a14dcf466e85730eb8be498bb/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[['sepal_length','sepal_width']]\n",
    "Y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, xts, ytr, yts = train_test_split(X,Y,test_size=0.30,random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr.shape, xts.shape, ytr.shape, yts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(xtr,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.score(xtr,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.score(xts,yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model3.predict(xts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred_y,yts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.DataFrame()\n",
    "temp1['variable'] = ['sepal_length','sepal_width']\n",
    "temp1['VarImp'] = np.round(model3.feature_importances_,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.sort_index(by='VarImp',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variable\tVarImp\n",
    "#2\tpetal_length\t0.58\n",
    "#3\tpetal_width\t0.40\n",
    "#1\tsepal_width\t0.02\n",
    "#0\tsepal_length\t0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variable\tVarImp\n",
    "#0\tsepal_length\t0.64\n",
    "#1\tsepal_width\t0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias and variance is due to selection of samples and features as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three different ways to trade-off between bias and variance\n",
    "\n",
    "#1. Bootstrap Aggregation (Bagging)\n",
    "#2. Boosting \n",
    "#3. Stacking: Same Variant Stacking and Different Variant Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging: Bagging classifier, Random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5197, 11), (5197,), (1300, 11), (1300,))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = BaggingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
       "         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
       "         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861458533769483"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.score(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6461538461538462"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 8, 'max_samples': 0.9, 'n_estimators': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [50,100,150,200,250],\n",
    "             \"max_samples\": [0.7,0.8,0.9],\n",
    "             \"max_features\": [5,6,7,8]}\n",
    "\n",
    "top_para = GridSearch_BestParam(Xtrain,Ytrain, model5, param_grid, cv=10)\n",
    "\n",
    "dd = pd.DataFrame(top_para)[['rank_test_score','params']]\n",
    "\n",
    "paramCV = dd[dd.rank_test_score==1].values.flatten()[1]\n",
    "print(paramCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=8, max_samples=0.9,\n",
       "         n_estimators=150, n_jobs=None, oob_score=False, random_state=None,\n",
       "         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dtmodel = BaggingClassifier(n_estimators=paramCV['n_estimators'], \n",
    "                                           max_samples=paramCV['max_samples'],\n",
    "                                    max_features=paramCV['max_features'])\n",
    "best_dtmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=8, max_samples=0.9,\n",
       "         n_estimators=150, n_jobs=None, oob_score=False, random_state=None,\n",
       "         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dtmodel.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.683076923076923\n"
     ]
    }
   ],
   "source": [
    "print(best_dtmodel.score(Xtrain,Ytrain))\n",
    "print(best_dtmodel.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9863382720800462\n",
      "0.65\n"
     ]
    }
   ],
   "source": [
    "print(model7.score(Xtrain,Ytrain))\n",
    "print(model7.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 9, 'max_features': 5, 'n_estimators': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [50,100,150,200,250],\n",
    "              \"criterion\":['gini','entropy'],\n",
    "             \"max_depth\": [5,7,8,9],\n",
    "             \"max_features\": [4,5,6,7]}\n",
    "\n",
    "top_para = GridSearch_BestParam(Xtrain,Ytrain, model7, param_grid, cv=10)\n",
    "\n",
    "dd = pd.DataFrame(top_para)[['rank_test_score','params']]\n",
    "\n",
    "paramCV = dd[dd.rank_test_score==1].values.flatten()[1]\n",
    "print(paramCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 12, 'max_features': 5, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [100,150,200],\n",
    "              \"criterion\":['gini','entropy'],\n",
    "             \"max_depth\": [9,10,12],\n",
    "             \"max_features\": [4,5]}\n",
    "\n",
    "top_para = GridSearch_BestParam(Xtrain,Ytrain, model7, param_grid, cv=5)\n",
    "\n",
    "dd = pd.DataFrame(top_para)[['rank_test_score','params']]\n",
    "\n",
    "paramCV = dd[dd.rank_test_score==1].values.flatten()[1]\n",
    "print(paramCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=12, max_features=5, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = RandomForestClassifier(n_estimators=paramCV['n_estimators'], \n",
    "                                           max_depth=paramCV['max_depth'],\n",
    "                                    max_features=paramCV['max_features'],\n",
    "                                criterion=paramCV['criterion'])\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=12, max_features=5, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9559361169905715\n",
      "0.67\n"
     ]
    }
   ],
   "source": [
    "print(best_model.score(Xtrain,Ytrain))\n",
    "print(best_model.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                  Train\tTest\tDiff\t\n",
    "#  Logistic regression\tModel 1\t54%\t54%\t0%\t\n",
    "#                                     Model 2\t53%\t53%\t1%\t\n",
    "#  Decision Tree\t        Model 3\t100%\t59%\t41%\t\n",
    "#                                     Model 4\t62%\t56%\t6%\t\n",
    "#  Bagging\t                   Model 5\t99%\t64%\t35%\t\n",
    "#                                     Model 6\t100%\t68%\t32%\t\n",
    "#  Randomforest\t          Model7\t99%\t65%\t34%\t\n",
    "#                                      Model8\t79%\t62%\t17%\tBias/Variance trade off is optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VarImp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volatile acidity</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>density</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>free sulfur dioxide</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>residual sugar</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chlorides</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total sulfur dioxide</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pH</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sulphates</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fixed acidity</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>citric acid</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                variable  VarImp\n",
       "10               alcohol    0.17\n",
       "1       volatile acidity    0.11\n",
       "7                density    0.10\n",
       "5    free sulfur dioxide    0.09\n",
       "3         residual sugar    0.08\n",
       "4              chlorides    0.08\n",
       "6   total sulfur dioxide    0.08\n",
       "8                     pH    0.08\n",
       "9              sulphates    0.08\n",
       "0          fixed acidity    0.07\n",
       "2            citric acid    0.07"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp['variable'] = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
    "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "       'pH', 'sulphates', 'alcohol']\n",
    "temp['VarImp'] = np.round(best_model.feature_importances_,2)\n",
    "\n",
    "temp.sort_index(by='VarImp',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:\n",
    "# create dummy features and train random forest classifier on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffa = pd.get_dummies(pd.qcut(df['fixed acidity'],q=5),prefix='fixed acidity',drop_first=True)\n",
    "dfva = pd.get_dummies(pd.qcut(df['volatile acidity'],q=5),prefix='volatile acidity',drop_first=True)\n",
    "dfca = pd.get_dummies(pd.qcut(df['citric acid'],q=5),prefix='citric acid',drop_first=True)\n",
    "dfrs = pd.get_dummies(pd.qcut(df['residual sugar'],q=5),prefix='residual sugar',drop_first=True)\n",
    "dfc = pd.get_dummies(pd.qcut(df['chlorides'],q=5),prefix='chlorides',drop_first=True)\n",
    "dffsd = pd.get_dummies(pd.qcut(df['free sulfur dioxide'],q=5),prefix='free sulfur dioxide',drop_first=True)\n",
    "dftsd = pd.get_dummies(pd.qcut(df['total sulfur dioxide'],q=5),prefix='total sulfur dioxide',drop_first=True)\n",
    "dfd = pd.get_dummies(pd.qcut(df['density'],q=5),prefix='density',drop_first=True)\n",
    "dfph = pd.get_dummies(pd.qcut(df['pH'],q=5),prefix='pH',drop_first=True)\n",
    "dfs = pd.get_dummies(pd.qcut(df['sulphates'],q=5),prefix='sulphates',drop_first=True)\n",
    "dfa = pd.get_dummies(pd.qcut(df['alcohol'],q=5),prefix='alcohol',drop_first=True)\n",
    "dfq = df.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([dffa,dfva,dfca,dfrs,dfc,dffsd,dftsd,dfd,dfph,dfs,dfa,dfq],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity_(6.3, 6.8]</th>\n",
       "      <th>fixed acidity_(6.8, 7.2]</th>\n",
       "      <th>fixed acidity_(7.2, 7.9]</th>\n",
       "      <th>fixed acidity_(7.9, 15.9]</th>\n",
       "      <th>volatile acidity_(0.21, 0.27]</th>\n",
       "      <th>volatile acidity_(0.27, 0.32]</th>\n",
       "      <th>volatile acidity_(0.32, 0.45]</th>\n",
       "      <th>volatile acidity_(0.45, 1.58]</th>\n",
       "      <th>citric acid_(0.23, 0.29]</th>\n",
       "      <th>citric acid_(0.29, 0.34]</th>\n",
       "      <th>...</th>\n",
       "      <th>pH_(3.35, 4.01]</th>\n",
       "      <th>sulphates_(0.41, 0.48]</th>\n",
       "      <th>sulphates_(0.48, 0.54]</th>\n",
       "      <th>sulphates_(0.54, 0.63]</th>\n",
       "      <th>sulphates_(0.63, 2.0]</th>\n",
       "      <th>alcohol_(9.4, 9.9]</th>\n",
       "      <th>alcohol_(9.9, 10.7]</th>\n",
       "      <th>alcohol_(10.7, 11.5]</th>\n",
       "      <th>alcohol_(11.5, 14.9]</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity_(6.3, 6.8]  fixed acidity_(6.8, 7.2]  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   fixed acidity_(7.2, 7.9]  fixed acidity_(7.9, 15.9]  \\\n",
       "0                         1                          0   \n",
       "1                         1                          0   \n",
       "2                         1                          0   \n",
       "3                         0                          1   \n",
       "4                         1                          0   \n",
       "\n",
       "   volatile acidity_(0.21, 0.27]  volatile acidity_(0.27, 0.32]  \\\n",
       "0                              0                              0   \n",
       "1                              0                              0   \n",
       "2                              0                              0   \n",
       "3                              0                              1   \n",
       "4                              0                              0   \n",
       "\n",
       "   volatile acidity_(0.32, 0.45]  volatile acidity_(0.45, 1.58]  \\\n",
       "0                              0                              1   \n",
       "1                              0                              1   \n",
       "2                              0                              1   \n",
       "3                              0                              0   \n",
       "4                              0                              1   \n",
       "\n",
       "   citric acid_(0.23, 0.29]  citric acid_(0.29, 0.34]  ...  pH_(3.35, 4.01]  \\\n",
       "0                         0                         0  ...                1   \n",
       "1                         0                         0  ...                0   \n",
       "2                         0                         0  ...                0   \n",
       "3                         0                         0  ...                0   \n",
       "4                         0                         0  ...                1   \n",
       "\n",
       "   sulphates_(0.41, 0.48]  sulphates_(0.48, 0.54]  sulphates_(0.54, 0.63]  \\\n",
       "0                       0                       0                       1   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       1   \n",
       "4                       0                       0                       1   \n",
       "\n",
       "   sulphates_(0.63, 2.0]  alcohol_(9.4, 9.9]  alcohol_(9.9, 10.7]  \\\n",
       "0                      0                   0                    0   \n",
       "1                      1                   1                    0   \n",
       "2                      1                   1                    0   \n",
       "3                      0                   1                    0   \n",
       "4                      0                   0                    0   \n",
       "\n",
       "   alcohol_(10.7, 11.5]  alcohol_(11.5, 14.9]  quality  \n",
       "0                     0                     0        5  \n",
       "1                     0                     0        5  \n",
       "2                     0                     0        5  \n",
       "3                     0                     0        6  \n",
       "4                     0                     0        5  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_new.pop('quality')\n",
    "X = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(X,y, test_size=0.30,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4547, 44), (1950, 44), (4547,), (1950,))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, xtest.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 12, 'max_features': 4, 'n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [100,150,200],\n",
    "              \"criterion\":['gini','entropy'],\n",
    "             \"max_depth\": [9,10,12],\n",
    "             \"max_features\": [4,5]}\n",
    "\n",
    "top_para = GridSearch_BestParam(xtrain,ytrain, model8, param_grid, cv=5)\n",
    "\n",
    "dd = pd.DataFrame(top_para)[['rank_test_score','params']]\n",
    "\n",
    "paramCV = dd[dd.rank_test_score==1].values.flatten()[1]\n",
    "print(paramCV)\n",
    "\n",
    "best_model = RandomForestClassifier(n_estimators=paramCV['n_estimators'], \n",
    "                                           max_depth=paramCV['max_depth'],\n",
    "                                    max_features=paramCV['max_features'],\n",
    "                                criterion=paramCV['criterion'])\n",
    "best_model\n",
    "\n",
    "best_model.fit(xtrain,ytrain)\n",
    "\n",
    "print(best_model.score(xtrain,ytrain))\n",
    "print(best_model.score(xtest,ytest))\n",
    "\n",
    "df_new.columns\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['variable'] = ['fixed acidity_(6.3, 6.8]', 'fixed acidity_(6.8, 7.2]',\n",
    "       'fixed acidity_(7.2, 7.9]', 'fixed acidity_(7.9, 15.9]',\n",
    "       'volatile acidity_(0.21, 0.27]', 'volatile acidity_(0.27, 0.32]',\n",
    "       'volatile acidity_(0.32, 0.45]', 'volatile acidity_(0.45, 1.58]',\n",
    "       'citric acid_(0.23, 0.29]', 'citric acid_(0.29, 0.34]',\n",
    "       'citric acid_(0.34, 0.42]', 'citric acid_(0.42, 1.66]',\n",
    "       'residual sugar_(1.7, 2.3]', 'residual sugar_(2.3, 5.0]',\n",
    "       'residual sugar_(5.0, 9.6]', 'residual sugar_(9.6, 65.8]',\n",
    "       'chlorides_(0.036, 0.044]', 'chlorides_(0.044, 0.051]',\n",
    "       'chlorides_(0.051, 0.073]', 'chlorides_(0.073, 0.611]',\n",
    "       'free sulfur dioxide_(15.0, 24.0]', 'free sulfur dioxide_(24.0, 33.0]',\n",
    "       'free sulfur dioxide_(33.0, 45.0]', 'free sulfur dioxide_(45.0, 289.0]',\n",
    "       'total sulfur dioxide_(62.2, 105.0]',\n",
    "       'total sulfur dioxide_(105.0, 132.0]',\n",
    "       'total sulfur dioxide_(132.0, 165.0]',\n",
    "       'total sulfur dioxide_(165.0, 440.0]', 'density_(0.992, 0.994]',\n",
    "       'density_(0.994, 0.996]', 'density_(0.996, 0.997]',\n",
    "       'density_(0.997, 1.039]', 'pH_(3.08, 3.17]', 'pH_(3.17, 3.25]',\n",
    "       'pH_(3.25, 3.35]', 'pH_(3.35, 4.01]', 'sulphates_(0.41, 0.48]',\n",
    "       'sulphates_(0.48, 0.54]', 'sulphates_(0.54, 0.63]',\n",
    "       'sulphates_(0.63, 2.0]', 'alcohol_(9.4, 9.9]', 'alcohol_(9.9, 10.7]',\n",
    "       'alcohol_(10.7, 11.5]', 'alcohol_(11.5, 14.9]']\n",
    "temp['VarImp'] = np.round(best_model.feature_importances_,2)\n",
    "\n",
    "temp.sort_index(by='VarImp',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9 = AdaBoostClassifier()\n",
    "model9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.5139652518143831\n",
      "0.4948717948717949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VarImp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>alcohol_(11.5, 14.9]</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>free sulfur dioxide_(33.0, 45.0]</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>free sulfur dioxide_(45.0, 289.0]</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>total sulfur dioxide_(165.0, 440.0]</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>volatile acidity_(0.45, 1.58]</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>alcohol_(10.7, 11.5]</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>free sulfur dioxide_(24.0, 33.0]</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>citric acid_(0.29, 0.34]</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chlorides_(0.073, 0.611]</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sulphates_(0.63, 2.0]</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>chlorides_(0.044, 0.051]</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>density_(0.996, 0.997]</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>citric acid_(0.23, 0.29]</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>residual sugar_(9.6, 65.8]</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>free sulfur dioxide_(15.0, 24.0]</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sulphates_(0.54, 0.63]</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>residual sugar_(1.7, 2.3]</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>alcohol_(9.4, 9.9]</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>volatile acidity_(0.32, 0.45]</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>total sulfur dioxide_(62.2, 105.0]</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>density_(0.992, 0.994]</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>total sulfur dioxide_(105.0, 132.0]</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>pH_(3.17, 3.25]</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>pH_(3.08, 3.17]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sulphates_(0.41, 0.48]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pH_(3.25, 3.35]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pH_(3.35, 4.01]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>sulphates_(0.48, 0.54]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>alcohol_(9.9, 10.7]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>density_(0.997, 1.039]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fixed acidity_(7.2, 7.9]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>density_(0.994, 0.996]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>total sulfur dioxide_(132.0, 165.0]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fixed acidity_(7.9, 15.9]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fixed acidity_(6.8, 7.2]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>chlorides_(0.051, 0.073]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>chlorides_(0.036, 0.044]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>residual sugar_(5.0, 9.6]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>residual sugar_(2.3, 5.0]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>citric acid_(0.42, 1.66]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>citric acid_(0.34, 0.42]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>volatile acidity_(0.27, 0.32]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>volatile acidity_(0.21, 0.27]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fixed acidity_(6.3, 6.8]</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               variable  VarImp\n",
       "43                 alcohol_(11.5, 14.9]    0.12\n",
       "22     free sulfur dioxide_(33.0, 45.0]    0.09\n",
       "23    free sulfur dioxide_(45.0, 289.0]    0.08\n",
       "27  total sulfur dioxide_(165.0, 440.0]    0.08\n",
       "7         volatile acidity_(0.45, 1.58]    0.07\n",
       "42                 alcohol_(10.7, 11.5]    0.06\n",
       "21     free sulfur dioxide_(24.0, 33.0]    0.06\n",
       "9              citric acid_(0.29, 0.34]    0.06\n",
       "19             chlorides_(0.073, 0.611]    0.06\n",
       "39                sulphates_(0.63, 2.0]    0.06\n",
       "17             chlorides_(0.044, 0.051]    0.04\n",
       "30               density_(0.996, 0.997]    0.04\n",
       "8              citric acid_(0.23, 0.29]    0.03\n",
       "15           residual sugar_(9.6, 65.8]    0.02\n",
       "20     free sulfur dioxide_(15.0, 24.0]    0.02\n",
       "38               sulphates_(0.54, 0.63]    0.02\n",
       "12            residual sugar_(1.7, 2.3]    0.02\n",
       "40                   alcohol_(9.4, 9.9]    0.02\n",
       "6         volatile acidity_(0.32, 0.45]    0.02\n",
       "24   total sulfur dioxide_(62.2, 105.0]    0.01\n",
       "28               density_(0.992, 0.994]    0.01\n",
       "25  total sulfur dioxide_(105.0, 132.0]    0.01\n",
       "33                      pH_(3.17, 3.25]    0.01\n",
       "32                      pH_(3.08, 3.17]    0.00\n",
       "36               sulphates_(0.41, 0.48]    0.00\n",
       "34                      pH_(3.25, 3.35]    0.00\n",
       "35                      pH_(3.35, 4.01]    0.00\n",
       "37               sulphates_(0.48, 0.54]    0.00\n",
       "41                  alcohol_(9.9, 10.7]    0.00\n",
       "31               density_(0.997, 1.039]    0.00\n",
       "2              fixed acidity_(7.2, 7.9]    0.00\n",
       "29               density_(0.994, 0.996]    0.00\n",
       "26  total sulfur dioxide_(132.0, 165.0]    0.00\n",
       "3             fixed acidity_(7.9, 15.9]    0.00\n",
       "1              fixed acidity_(6.8, 7.2]    0.00\n",
       "18             chlorides_(0.051, 0.073]    0.00\n",
       "16             chlorides_(0.036, 0.044]    0.00\n",
       "14            residual sugar_(5.0, 9.6]    0.00\n",
       "13            residual sugar_(2.3, 5.0]    0.00\n",
       "11             citric acid_(0.42, 1.66]    0.00\n",
       "10             citric acid_(0.34, 0.42]    0.00\n",
       "5         volatile acidity_(0.27, 0.32]    0.00\n",
       "4         volatile acidity_(0.21, 0.27]    0.00\n",
       "0              fixed acidity_(6.3, 6.8]    0.00"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [100,150,200],\n",
    "              \"learning_rate\": [0.05,0.10]}\n",
    "\n",
    "top_para = GridSearch_BestParam(xtrain,ytrain, model9, param_grid, cv=5)\n",
    "\n",
    "dd = pd.DataFrame(top_para)[['rank_test_score','params']]\n",
    "\n",
    "paramCV = dd[dd.rank_test_score==1].values.flatten()[1]\n",
    "print(paramCV)\n",
    "\n",
    "best_model = AdaBoostClassifier(n_estimators=paramCV['n_estimators'], \n",
    "                                           learning_rate=paramCV['learning_rate'])\n",
    "best_model\n",
    "\n",
    "best_model.fit(xtrain,ytrain)\n",
    "\n",
    "print(best_model.score(xtrain,ytrain))\n",
    "print(best_model.score(xtest,ytest))\n",
    "\n",
    "df_new.columns\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['variable'] = ['fixed acidity_(6.3, 6.8]', 'fixed acidity_(6.8, 7.2]',\n",
    "       'fixed acidity_(7.2, 7.9]', 'fixed acidity_(7.9, 15.9]',\n",
    "       'volatile acidity_(0.21, 0.27]', 'volatile acidity_(0.27, 0.32]',\n",
    "       'volatile acidity_(0.32, 0.45]', 'volatile acidity_(0.45, 1.58]',\n",
    "       'citric acid_(0.23, 0.29]', 'citric acid_(0.29, 0.34]',\n",
    "       'citric acid_(0.34, 0.42]', 'citric acid_(0.42, 1.66]',\n",
    "       'residual sugar_(1.7, 2.3]', 'residual sugar_(2.3, 5.0]',\n",
    "       'residual sugar_(5.0, 9.6]', 'residual sugar_(9.6, 65.8]',\n",
    "       'chlorides_(0.036, 0.044]', 'chlorides_(0.044, 0.051]',\n",
    "       'chlorides_(0.051, 0.073]', 'chlorides_(0.073, 0.611]',\n",
    "       'free sulfur dioxide_(15.0, 24.0]', 'free sulfur dioxide_(24.0, 33.0]',\n",
    "       'free sulfur dioxide_(33.0, 45.0]', 'free sulfur dioxide_(45.0, 289.0]',\n",
    "       'total sulfur dioxide_(62.2, 105.0]',\n",
    "       'total sulfur dioxide_(105.0, 132.0]',\n",
    "       'total sulfur dioxide_(132.0, 165.0]',\n",
    "       'total sulfur dioxide_(165.0, 440.0]', 'density_(0.992, 0.994]',\n",
    "       'density_(0.994, 0.996]', 'density_(0.996, 0.997]',\n",
    "       'density_(0.997, 1.039]', 'pH_(3.08, 3.17]', 'pH_(3.17, 3.25]',\n",
    "       'pH_(3.25, 3.35]', 'pH_(3.35, 4.01]', 'sulphates_(0.41, 0.48]',\n",
    "       'sulphates_(0.48, 0.54]', 'sulphates_(0.54, 0.63]',\n",
    "       'sulphates_(0.63, 2.0]', 'alcohol_(9.4, 9.9]', 'alcohol_(9.9, 10.7]',\n",
    "       'alcohol_(10.7, 11.5]', 'alcohol_(11.5, 14.9]']\n",
    "temp['VarImp'] = np.round(best_model.feature_importances_,2)\n",
    "\n",
    "temp.sort_index(by='VarImp',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GradientBoostingClassifier in module sklearn.ensemble.gradient_boosting object:\n",
      "\n",
      "class GradientBoostingClassifier(BaseGradientBoosting, sklearn.base.ClassifierMixin)\n",
      " |  GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n",
      " |  \n",
      " |  Gradient Boosting for classification.\n",
      " |  \n",
      " |  GB builds an additive model in a\n",
      " |  forward stage-wise fashion; it allows for the optimization of\n",
      " |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      " |  regression trees are fit on the negative gradient of the\n",
      " |  binomial or multinomial deviance loss function. Binary classification\n",
      " |  is a special case where only a single regression tree is induced.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'deviance', 'exponential'}, optional (default='deviance')\n",
      " |      loss function to be optimized. 'deviance' refers to\n",
      " |      deviance (= logistic regression) for classification\n",
      " |      with probabilistic outputs. For loss 'exponential' gradient\n",
      " |      boosting recovers the AdaBoost algorithm.\n",
      " |  \n",
      " |  learning_rate : float, optional (default=0.1)\n",
      " |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int (default=100)\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  subsample : float, optional (default=1.0)\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"friedman_mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"friedman_mse\" for the mean squared error with improvement\n",
      " |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      " |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      " |      generally the best as it can provide a better approximation in\n",
      " |      some cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_depth : integer, optional (default=3)\n",
      " |      maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  init : estimator, optional\n",
      " |      An estimator object that is used to compute the initial\n",
      " |      predictions. ``init`` has to provide ``fit`` and ``predict``.\n",
      " |      If None it uses ``loss.init_estimator``.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=None)\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  verbose : int, default: 0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default: False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  presort : bool or 'auto', optional (default='auto')\n",
      " |      Whether to presort the data to speed up the finding of best splits in\n",
      " |      fitting. Auto mode by default will use presorting on dense data and\n",
      " |      default to normal sorting on sparse data. Setting presort to true on\n",
      " |      sparse data will raise an error.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *presort* parameter.\n",
      " |  \n",
      " |  validation_fraction : float, optional, default 0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, optional, default 1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_estimators_ : int\n",
      " |      The number of estimators as selected by early stopping (if\n",
      " |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  feature_importances_ : array, shape (n_features,)\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_improvement_ : array, shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |  \n",
      " |  train_score_ : array, shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor,shape (n_estimators, ``loss_.K``)\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n",
      " |  AdaBoostClassifier\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingClassifier\n",
      " |      BaseGradientBoosting\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : array, shape (n_samples, n_classes) or (n_samples,)\n",
      " |          The decision function of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |          Regression and binary classification produce an array of shape\n",
      " |          [n_samples].\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array, shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array, shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array, shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : generator of array, shape (n_samples, k)\n",
      " |          The decision function of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |          Regression and binary classification are special cases with\n",
      " |          ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict class at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of array of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of array of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like, shape (n_samples, n_estimators, n_classes)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |          In the case of binary classification n_classes is 1.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,) or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, optional\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape (n_features,)\n",
      " |  \n",
      " |  n_features\n",
      " |      DEPRECATED: Attribute n_features was deprecated in version 0.19 and will be removed in 0.21.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-f190208ecf9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     }\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtop_para\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearch_BestParam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_para\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rank_test_score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-1b03bed5bf82>\u001b[0m in \u001b[0;36mGridSearch_BestParam\u001b[1;34m(X, y, clf, param_grid, cv)\u001b[0m\n\u001b[0;32m      5\u001b[0m                               cv=cv)\n\u001b[0;32m      6\u001b[0m     \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtop_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtop_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1463\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n\u001b[0;32m   1464\u001b[0m                                     \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1465\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1467\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1527\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1528\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m   1192\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m-> 1194\u001b[1;33m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {\"criterion\":['friedman_mse','mae','mse'],\n",
    "\"learning_rate\":[0.1,0.5,0.05,0.005], \n",
    "\"max_depth\":[3,5,7],\n",
    "\"max_features\":[5,8,10,12,None], \n",
    "\"n_estimators\":[100,200,500,750,1000]\n",
    "    }\n",
    "\n",
    "top_para = GridSearch_BestParam(xtrain,ytrain, model10, param_grid, cv=5)\n",
    "\n",
    "dd = pd.DataFrame(top_para)[['rank_test_score','params']]\n",
    "\n",
    "paramCV = dd[dd.rank_test_score==1].values.flatten()[1]\n",
    "print(paramCV)\n",
    "\n",
    "best_model = GradientBoostingClassifier(n_estimators=paramCV['n_estimators'],\n",
    "                                learning_rate=paramCV['learning_rate'],\n",
    "                               criterion = paramCV['criterion'],\n",
    "                              max_depth=paramCV['max_depth'],\n",
    "                               max_features=paramCV['max_features'])\n",
    "best_model\n",
    "\n",
    "best_model.fit(xtrain,ytrain)\n",
    "\n",
    "print(best_model.score(xtrain,ytrain))\n",
    "print(best_model.score(xtest,ytest))\n",
    "\n",
    "df_new.columns\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['variable'] = ['fixed acidity_(6.3, 6.8]', 'fixed acidity_(6.8, 7.2]',\n",
    "       'fixed acidity_(7.2, 7.9]', 'fixed acidity_(7.9, 15.9]',\n",
    "       'volatile acidity_(0.21, 0.27]', 'volatile acidity_(0.27, 0.32]',\n",
    "       'volatile acidity_(0.32, 0.45]', 'volatile acidity_(0.45, 1.58]',\n",
    "       'citric acid_(0.23, 0.29]', 'citric acid_(0.29, 0.34]',\n",
    "       'citric acid_(0.34, 0.42]', 'citric acid_(0.42, 1.66]',\n",
    "       'residual sugar_(1.7, 2.3]', 'residual sugar_(2.3, 5.0]',\n",
    "       'residual sugar_(5.0, 9.6]', 'residual sugar_(9.6, 65.8]',\n",
    "       'chlorides_(0.036, 0.044]', 'chlorides_(0.044, 0.051]',\n",
    "       'chlorides_(0.051, 0.073]', 'chlorides_(0.073, 0.611]',\n",
    "       'free sulfur dioxide_(15.0, 24.0]', 'free sulfur dioxide_(24.0, 33.0]',\n",
    "       'free sulfur dioxide_(33.0, 45.0]', 'free sulfur dioxide_(45.0, 289.0]',\n",
    "       'total sulfur dioxide_(62.2, 105.0]',\n",
    "       'total sulfur dioxide_(105.0, 132.0]',\n",
    "       'total sulfur dioxide_(132.0, 165.0]',\n",
    "       'total sulfur dioxide_(165.0, 440.0]', 'density_(0.992, 0.994]',\n",
    "       'density_(0.994, 0.996]', 'density_(0.996, 0.997]',\n",
    "       'density_(0.997, 1.039]', 'pH_(3.08, 3.17]', 'pH_(3.17, 3.25]',\n",
    "       'pH_(3.25, 3.35]', 'pH_(3.35, 4.01]', 'sulphates_(0.41, 0.48]',\n",
    "       'sulphates_(0.48, 0.54]', 'sulphates_(0.54, 0.63]',\n",
    "       'sulphates_(0.63, 2.0]', 'alcohol_(9.4, 9.9]', 'alcohol_(9.9, 10.7]',\n",
    "       'alcohol_(10.7, 11.5]', 'alcohol_(11.5, 14.9]']\n",
    "temp['VarImp'] = np.round(best_model.feature_importances_,2)\n",
    "\n",
    "temp.sort_index(by='VarImp',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.615790631185397"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.score(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5384615384615384"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
